---
title: " Mind the Gap: Why Arabic LLMs Still Lag Behind (and What We Can Do About It)"
description: "This blog explores why Arabic large language models lag behind their English counterparts, highlighting gaps in post-training data quality, cultural alignment, and task diversity. It offers practical solutions to build authentic, high-impact Arabic datasets that empower better AI for Arabic speakers across dialects and domains."
date: 05-08-2025
authors:
  - avatar: "https://ui.shadcn.com/avatars/02.png"
    handle: reactdev
    username: Mohamed Zaytoon
    handleUrl: "https://github.com/reactdev"

cover: "https://img.freepik.com/premium-vector/many-monsters-various-colors-doodle-come-bless-birthday-happy_577083-85.jpg?w=826"
---

In training large language models (LLMs), **post-training** has become the secret sauce that turns powerful but generic language models into finely tuned assistants that actually understand what you want. While most people have heard of "training" and maybe even "fine-tuning," post-training is the magic layer that aligns a language model with real-world expectations: understanding your instructions, staying safe, and behaving like a good digital citizen.

For **Arabic**, that magic is still hard to come by.



## The Problem: Plenty of Models, Not Enough (Good) Data

Letâ€™s start with the good news. In recent years, weâ€™ve seen a wave of Arabic-centric LLMs, like **JAIS**, **AceGPT**, and the **Allam** series, enter the scene. These models are promising, but theyâ€™re still missing something crucial: **high-quality, culturally nuanced Arabic post-training datasets**.

Our recent study systematically reviewed all publicly available Arabic post-training datasets on **Hugging Face**. What we found was eye-opening:

* Over **360 datasets** are available, but the majority are skewed toward only two tasks:

  * **Translation (42%)**
  * **Question Answering (38%)**
* More complex or essential tasks like **function calling**, **code generation**, or **dialogue** have almost **no representation**.
* Over **50%** of the datasets are **rarely or never used** in real-world models.
* Only a handful are updated regularly or have **strong documentation**, **licensing**, or **academic validation**.



## Why This Matters

Imagine building a chatbot for an Arabic-speaking audience. If all your data is translated from English, the bot might miss the **tone**, **context**, or even **offend users unintentionally**. Thatâ€™s not just awkwardâ€”it can be **dangerous** in sensitive applications like **education**, **healthcare**, or **civic services**.

### Three big problems stood out:

1. **Cultural Misalignment**
   Many datasets are translated from English without cultural filtering or adaptation. This leads to models that donâ€™t "speak" or "think" like native Arabic speakers.

2. **Low Dataset Quality & Visibility**
   Most Arabic datasets lack clear documentation, proper licensing, or validation. Without that, other researchers canâ€™t easily reuse or trust them.

3. **Missing Support for Advanced Tasks**
   Code generation, system prompting, and robust function calling? Arabic datasets for these are either **missing** or **experimental** at best.



## What We Found

We analyzed Arabic datasets across **12 NLP domains**. Here's a snapshot of where things stand:

| **Task Domain**                                                                    | **Current Coverage**   | **Gap?**         |
| ---------------------------------------------------------------------------------- | ---------------------- | ---------------- |
| Translation, Q\&A                                                                  | Strong (150+ each)     | None             |
| Summarization                                                                      | Moderate (45)          | Quality issues   |
| Reasoning & Multi-step, Dialog/Conversation, Robustness & Safety                   | Sparse (8 datasets)    | Needs more scale |
| Cultural Alignment, Ethics, Bias, and Fairness                                     | Very limited (â‰¤3 each) | Critical gap     |
| Code Generation, Function Call, Official Docs, and Persona/Ownership/System Prompt | None                   | Total absence    |

> Only **57 out of 366 datasets** are used in actual models. The rest sit unused, untested, or unnoticed.

![](/en/mind_the_gap_1.png)


## What We Need (And How to Build It)

If we want Arabic LLMs to be as good, or better, than their English counterparts, we need to change how we build post-training datasets. That starts with building for **authenticity**, **culture**, and **complexity**.

### Hereâ€™s what we recommend:

#### ðŸš« Stop Translating. âœ… Start Creating.

Donâ€™t just translate English data. Instead, crowdsource or generate **native Arabic data** that reflects real-life use cases, dialects, and values.

#### ðŸŽ¯ Target the Gaps

Focus on the domains where Arabic is critically underrepresented:

* Code generation
* Function/API calling
* Culturally aligned Q\&A
* Dialogues in regional dialects
* Safety and ethical alignment

#### ðŸ§  Use Smarter Methods

* Scrape Arabic GitHub repos to create **code-comment pairs**
* Collect **dialectal conversations** from native speakers
* Launch **collaborative annotation platforms** for cultural labeling
* Use LLMs for **first-pass annotation**, followed by **human review**
* Prompt existing Arabic-capable LLMs to generate **syntheticâ€”but verifiedâ€”data**



## Letâ€™s Build Together

Building strong Arabic LLMs is more than a research goal, itâ€™s a **cultural mission**. We have the models, but we need better data: **authentic, representative, and task-specific**. Thatâ€™s where the next wave of innovation lies.

So whether youâ€™re a **researcher**, a **developer**, or a **language lover**, consider this your **call to action**. Letâ€™s create the datasets that Arabic deservesâ€”**rich in dialects**, **deep in meaning**, and **powerful enough** to unlock the full potential of Arabic AI.

---

### Reference

*Study based on analysis of Arabic datasets on [Hugging Face](https://huggingface.co/).*

