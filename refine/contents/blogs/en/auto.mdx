---
title: "Automatic Instruction Evolving for Large Language Models"
description: "Explore how to leverage React Server Components and Server Actions in Next.js to build modern, efficient web applications. Learn how these features enhance performance and simplify server-side logic."
date: 01-08-2025
authors:
  - avatar: "https://ui.shadcn.com/avatars/02.png"
    handle: auto
    username: Mohamed Zaytoon
    handleUrl: "https://github.com/reactdev"

cover: "https://img.freepik.com/premium-vector/many-monsters-various-colors-doodle-come-bless-birthday-happy_577083-85.jpg?w=826"
---

# **Paper Information**

**Title**: *Automatic Instruction Evolving for Large Language Models*

**Authors**: Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, Weizhu Chen

**Affiliation**: Microsoft

**Contact**: {v-weihaozeng, caxu, v-yingxizhao, jlou, wzchen}@microsoft.com

**Published**: June 2, 2024

**arXiv**: [arXiv:2406.00770v1](https://arxiv.org/abs/2406.00770)

---

# **Introduction**

The challenge of instruction tuning has emerged as a critical aspect in advancing large language models (LLMs). Instruction tuning involves fine-tuning LLMs to follow complex instructions, which enhances their ability to perform specific tasks with high accuracy. However, the process is resource-intensive, requiring expert human intervention to design and annotate instruction datasets. Despite efforts to automate this process, scaling high-quality instruction data while maintaining diversity and complexity has proven difficult.

Large Language Models (LLMs) play a pivotal role in the evolution of instruction tuning. These models have shown remarkable potential in transforming simple instruction sets into complex datasets that are better suited to challenging tasks like code generation and mathematical reasoning. By utilizing LLMs, researchers have managed to improve instruction-following capabilities, enhancing the models' performance across various domains.

To address the bottlenecks of human-dependent instruction design, the paper introduces Auto Evol-Instruct. This innovative framework automates the process of evolving instruction datasets, eliminating the need for human intervention. By leveraging the power of LLMs, Auto Evol-Instruct autonomously analyzes, evolves, and refines instruction data to improve complexity, diversity, and model alignment. This automated approach outperforms traditional methods, showcasing significant advancements in instruction evolution, and holds promise for more scalable and adaptable instruction-tuning processes.

---

# **Background**

As large language models (LLMs) continue to evolve, instruction tuning has emerged as a key method to improve their alignment with human intent. Among the recent innovations in this space is Evol-Instruct, a framework that enhances instruction datasets by iteratively refining them to be more complex and diverse. Developed by human experts, Evol-Instruct applies a set of manually designed transformation rules to high-quality seed instructions. These evolved instructions aim to challenge the model more effectively, improving its ability to generalize and perform across a broader spectrum of tasks.

Evolving instructions is beneficial because it enables the creation of richer and more varied training data without requiring entirely new datasets. By refining existing instructions into more nuanced or difficult forms, models trained on this data become better at handling real-world complexity. This has been shown to improve model performance across domains such as code generation, mathematical reasoning, and multi-turn dialogue.

However, Evol-Instruct’s reliance on manual method design introduces several constraints. Each domain (e.g., coding vs. conversational tasks) requires domain-specific evolution rules, which must be crafted and validated by human experts. This high dependence on human labor not only slows down the development cycle but also limits scalability. When applied to new tasks or data domains, the evolution strategy must often be redesigned from scratch, making the process costly and difficult to generalize.

These limitations have sparked interest in automating the instruction evolution process, laying the foundation for Auto Evol-Instruct—a fully automated framework that retains the benefits of Evol-Instruct while eliminating its manual bottlenecks.

---

# **The Auto Evol-Instruct Framework**

Auto Evol-Instruct is a fully automated instruction evolution framework that removes the need for human-curated seed instructions or hand-designed transformation rules. It operates in a loop of evolution, trajectory analysis, and optimization, driven entirely by large language models (LLMs).

The overall workflow is illustrated in following **Figure 1**, which shows how the evolving method is iteratively refined based on feedback from instruction trajectories and multiple candidate prompts.


## **a. Initial Evolving Method**

The Auto Evol-Instruct framework begins with a **general-purpose, domain-agnostic prompt**—referred to as the **initial evolving method**—designed to instruct the LLM to transform basic instructions into more **complex, diverse, and thought-provoking** ones.

This is a crucial departure from earlier approaches like Evol-Instruct, which required **manual design of domain-specific transformation rules**. Instead, Auto Evol-Instruct uses a single, universal prompt to initiate the evolution process. This allows it to handle a wide range of instruction types without any task-specific tailoring.

Here’s how it works:

1. At each optimization step $t$, the system **samples a fresh batch of instructions**, denoted $X_t$, from a larger pool of raw instructions. This resampling ensures that the evolving method is exposed to a **diverse and continuously changing set of instructions**, preventing overfitting and encouraging generalization across task types.
2. Each instruction in $X_t$ is passed through an **evolution LLM** using the universal prompt. The goal is to evolve the instruction into a more challenging form. For example:
    - *"List three benefits of drinking water."* → *"List and compare three scientific studies on the cognitive benefits of hydration in adults."*
    - *"Translate this sentence to Spanish: 'The dog is barking loudly.'"* → *"Translate the following short story to Spanish, preserving literary style: 'The dog barked into the silent night, unsettling even the stars.'"*
3. This initial transformation is guided by a universal prompt like the one shown below:


By leveraging this simple but powerful prompt, Auto Evol-Instruct eliminates the need for human-designed evolution strategies, enabling scalable and consistent instruction transformation across domains.

## **b. Evol Trajectory Analysis**

After several rounds of instruction evolution, Auto Evol-Instruct performs a critical evaluation step known as **trajectory analysis**. This step ensures that the evolved instructions are not just different—but actually better, in terms of complexity, diversity, and instructional quality.

The process works as follows:

1. At each optimization step $t$, the system begins with a batch of instructions, denoted $X_t$ 
    
    For example:
    
    - *"Translate the following sentence into French: 'The cat is sleeping on the sofa.'"*
    - *"Summarize this paragraph in one sentence."*
    - *"Write a Python function that returns the factorial of a number."*
2. Using the current evolving method $e_{t-1}$, these instructions are evolved over several rounds—say, 3 iterations—producing a sequence like:
    
    $$
    S_t = \{X_t^{(1)}, X_t^{(2)}, X_t^{(3)}\}
    $$
    
    Each $X_t^{(i)}$ is the output of applying one evolution round to the previous version. For example, the factorial instruction might evolve like this:
    
    - **Round 1**: *"Write a recursive Python function that returns the factorial of a number."*
    - **Round 2**: *"Write a recursive Python function to compute the factorial of a number and explain its time complexity."*
    - **Round 3**: *"Create a Python module with unit tests that verify a recursive factorial function under edge cases and large inputs."*
3. Once the full **evolutionary trajectory** is generated, the **optimizer LLM** analyzes it to assess the quality of these transformations. It looks for signs of improvement or degeneration using criteria such as:
    - **Increased reasoning steps** – Is the instruction requiring more cognitive effort from the model?
    - **More nuanced responses** – Has the instruction evolved to demand more detailed or sophisticated outputs?
    - **Task diversity and novelty** – Are the instructions becoming more varied or creative?
4. The optimizer LLM then generates **structured feedback** ftf_tft, identifying any issues such as stagnation, redundancy, or loss of clarity. This feedback is not merely qualitative—it’s used directly in the next stage to improve the evolving method itself.

This feedback is generated using a carefully designed prompt, shown below:


By examining how each instruction changes across rounds, the system builds a deeper understanding of how to evolve better instructions in future iterations.

## **c. Evolving Method Optimization**

After analyzing the evolution trajectories, the Auto Evol-Instruct framework proceeds to optimize the evolving method itself—that is, the prompt that guides the LLM in transforming instructions.

At this stage, the **optimizer LLM** takes in feedback from the previous round of evolution and uses it to revise the evolving method in a way that better aligns with the goal of producing complex, diverse, and meaningful instructions.

This is achieved using a dedicated **optimization prompt**, shown below, which instructs the optimizer LLM to consider the past performance and suggest targeted improvements:


### Multiple Optimization Trials

At each optimization step ttt, the optimizer LLM runs **mmm parallel trials** using sampling-based decoding to produce mmm distinct evolving method candidates:

$$
{{e^1_t, e^2_t,.... ,e^m_t}}
$$

These candidate methods are evaluated to determine which one most effectively improves the quality of instruction evolution.

Each evolving method $e^i_t$ is applied to a development set , producing an evolved response set $R^{e^i_t}$. To measure performance, the framework computes a **failure rate** based on a set of well-defined criteria, using a function $F(r)$, where:

- $F(r)=1$, if the evolved instruction is considered a failure.
- $F(r)=0$, if it’s considered a success.

The failure cases are defined according to three key scenarios:

1. **Stagnant Complexity**:
    
    The evolved instruction fails to show added complexity, essentially repeating or restating the original task. Responses to such prompts often begin with phrases like “Understood” or “Thank you,” and end with a question mark—indicating the model is confused or passively acknowledging the instruction.
    
2. **Insufficient Qualification**:
    
    The instruction lacks necessary details or constraints, forcing the LLM to ask for clarification before providing a useful response. Such responses often start with “Sure” and end in a question mark, reflecting uncertainty.
    
3. **Loss of Key Information**:
    
    The evolved instruction omits important content from the original, which prevents the LLM from generating a complete or meaningful answer. These cases are usually flagged by replies like “please provide” or other indications that critical context is missing.
    

The failure rate for each candidate method is computed as:

$$
\lambda^{R^{e^i_t}} = \frac{1}{|D|} \sum_{r \in R^{e^i_t}} F(r)
$$

Where $∣D∣$is the number of examples in the development set.

The evolving method with the **lowest failure rate** is selected as the final $e_t$ for the next round of instruction evolution.

---

# **Experimental Results**

Auto Evol-Instruct is evaluated on a range of standard instruction-following and reasoning benchmarks to assess improvements in instruction tuning performance:

## **Benchmarks Used**

- **MT-Bench**: A multi-turn evaluation framework for chat capabilities, using GPT-4 as a judge.
- **AlpacaEval**: Human-aligned evaluation for instruction-following ability.
- **GSM8K**: Grade-school math reasoning dataset to test arithmetic and logical reasoning.
- **HumanEval**: Python programming task benchmark used to evaluate code generation performance.

## **Key Metrics**

- Models trained on evolved data consistently outperform those trained on original data.
- On **MT-Bench**, Auto Evol-Instruct tuning improves win rates significantly over baseline and Evol-Instruct.
- On **GSM8K**, the evolved data fine-tuned model shows marked improvements in logical reasoning.
- In **HumanEval**, evolved data enhances performance by improving instruction clarity and constraints.

## **Result Table**



---

# **Discussion**

## **Why This Framework Matters**

- Auto Evol-Instruct represents a step forward in **instruction tuning automation**, drastically reducing human labor in prompt engineering.
- It enables **robust instruction datasets** that enhance generalization and alignment performance.

## **Real-World Potential**

- Can be used to scale up aligned datasets in domains like education, programming, or healthcare.
- Reduces dependency on expert human annotators for creating rich, challenging instructions.

---

# **Limitations & Future Work**

## Limitations include:

- **LLM Dependency**: Performance varies with the choice of evol and optimizer LLMs.
- **Prompt Design**: Despite automation, prompts can still degenerate or produce trivial results.
- **Evaluation Heuristics**: Rule-based failure detection may miss subtle flaws or overflag good evolutions.

## **Future Work Includes:**

- Expanding to **more LLM types and APIs**
- Applying the method to **broader domains**, including multi-modal instructions
- Improving **feedback evaluation** using learned models rather than hand-crafted heuristics

---

## **Conclusion**

Auto Evol-Instruct introduces a scalable, LLM-powered framework for evolving instructional data without human-written seeds or rule-based transformations. By combining evolution, feedback, and self-improving optimization loops, it achieves significant gains across several benchmarks.

This framework not only advances instruction tuning but also lays a foundation for more autonomous data curation in future LLM alignment pipelines.